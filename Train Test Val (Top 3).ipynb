{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "518a8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Loader for HAM10000 + BCN20000 Skin Lesion Datasets\n",
    "Handles the existing directory structure without reorganization\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from dataset import SkinLesionDataset\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "def load_ham10000_metadata(metadata_path, img_dir_part1, img_dir_part2):\n",
    "    \"\"\"Load HAM10000 metadata and create image path mappings\"\"\"\n",
    "    \n",
    "    # HAM10000 diagnosis code mapping\n",
    "    ham_dx_map = {\n",
    "        'nv': 'Nevus',\n",
    "        'mel': 'Melanoma (HAM)',\n",
    "        'bcc': 'Basal cell carcinoma',\n",
    "        'akiec': 'Actinic keratosis',\n",
    "        'vasc': 'Vascular lesion',\n",
    "        'df': 'Dermatofibroma',\n",
    "        'bkl': 'Benign keratosis (HAM)'\n",
    "    }\n",
    "    \n",
    "    # Try reading the metadata - could be CSV with or without header\n",
    "    try:\n",
    "        df = pd.read_csv(metadata_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading HAM metadata: {e}\")\n",
    "        return [], []\n",
    "    \n",
    "    print(f\"HAM metadata columns: {df.columns.tolist()}\")\n",
    "    print(f\"HAM metadata shape: {df.shape}\")\n",
    "    \n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Get image_id\n",
    "        if 'image_id' in df.columns:\n",
    "            image_id = row['image_id']\n",
    "        elif 'lesion_id' in df.columns:\n",
    "            image_id = row['lesion_id']\n",
    "        else:\n",
    "            image_id = row.iloc[0]\n",
    "        \n",
    "        # Get diagnosis code\n",
    "        if 'dx' in df.columns:\n",
    "            dx_code = row['dx']\n",
    "        elif 'diagnosis' in df.columns:\n",
    "            dx_code = row['diagnosis']\n",
    "        else:\n",
    "            dx_code = row.iloc[1]\n",
    "        \n",
    "        # Map to full diagnosis name\n",
    "        diagnosis = ham_dx_map.get(dx_code, dx_code)\n",
    "        \n",
    "        # Check both part1 and part2 directories\n",
    "        path1 = os.path.join(img_dir_part1, f\"{image_id}.jpg\")\n",
    "        path2 = os.path.join(img_dir_part2, f\"{image_id}.jpg\")\n",
    "        \n",
    "        if os.path.exists(path1):\n",
    "            image_paths.append(path1)\n",
    "            labels.append(diagnosis)\n",
    "        elif os.path.exists(path2):\n",
    "            image_paths.append(path2)\n",
    "            labels.append(diagnosis)\n",
    "        else:\n",
    "            print(f\"Warning: Image {image_id} not found\")\n",
    "    \n",
    "    print(f\"\\nHAM10000 diagnosis distribution:\")\n",
    "    for dx, count in pd.Series(labels).value_counts().items():\n",
    "        print(f\"  {dx}: {count}\")\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "def load_bcn20000_metadata(metadata_path, img_dir):\n",
    "    \"\"\"Load BCN20000 metadata - uses 'isic_id' and 'diagnosis_3' columns\"\"\"\n",
    "    \n",
    "    # BCN diagnosis mapping (standardize names)\n",
    "    bcn_diagnosis_map = {\n",
    "        'Nevus': 'Nevus',\n",
    "        'Melanoma, NOS': 'Melanoma (BCN)',\n",
    "        'Melanoma metastasis': 'Melanoma metastasis',\n",
    "        'Basal cell carcinoma': 'Basal cell carcinoma',\n",
    "        'Seborrheic keratosis': 'Seborrheic keratosis',\n",
    "        'Solar or actinic keratosis': 'Actinic keratosis',\n",
    "        'Squamous cell carcinoma, NOS': 'Squamous cell carcinoma',\n",
    "        'Scar': 'Scar',\n",
    "        'Solar lentigo': 'Solar lentigo',\n",
    "        'Dermatofibroma': 'Dermatofibroma'\n",
    "    }\n",
    "    \n",
    "    df = pd.read_csv(metadata_path)\n",
    "    \n",
    "    print(f\"\\nBCN20000 metadata shape: {df.shape}\")\n",
    "    print(f\"BCN20000 columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Verify expected columns exist\n",
    "    if 'isic_id' not in df.columns:\n",
    "        raise ValueError(f\"Expected 'isic_id' column in BCN metadata. Found: {df.columns.tolist()}\")\n",
    "    if 'diagnosis_3' not in df.columns:\n",
    "        raise ValueError(f\"Expected 'diagnosis_3' column in BCN metadata. Found: {df.columns.tolist()}\")\n",
    "    \n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    skipped_missing = 0\n",
    "    skipped_not_found = 0\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        image_id = row['isic_id']\n",
    "        raw_diagnosis = row['diagnosis_3']\n",
    "        \n",
    "        # Skip rows with missing diagnosis\n",
    "        if pd.isna(raw_diagnosis):\n",
    "            skipped_missing += 1\n",
    "            continue\n",
    "        \n",
    "        # Standardize diagnosis name\n",
    "        diagnosis = bcn_diagnosis_map.get(raw_diagnosis, raw_diagnosis)\n",
    "        \n",
    "        img_path = os.path.join(img_dir, f\"{image_id}.jpg\")\n",
    "        \n",
    "        if os.path.exists(img_path):\n",
    "            image_paths.append(img_path)\n",
    "            labels.append(diagnosis)\n",
    "        else:\n",
    "            skipped_not_found += 1\n",
    "    \n",
    "    print(f\"\\nBCN20000 loaded: {len(image_paths)} images\")\n",
    "    if skipped_missing > 0:\n",
    "        print(f\"  Skipped {skipped_missing} images with missing diagnosis ({skipped_missing/len(df)*100:.1f}%)\")\n",
    "    if skipped_not_found > 0:\n",
    "        print(f\"  Skipped {skipped_not_found} images not found on disk\")\n",
    "    \n",
    "    print(\"BCN20000 diagnosis distribution:\")\n",
    "    for dx, count in pd.Series(labels).value_counts().items():\n",
    "        print(f\"  {dx}: {count}\")\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "def create_data_loaders(\n",
    "    ham_metadata_path,\n",
    "    ham_img_part1,\n",
    "    ham_img_part2,\n",
    "    bcn_metadata_path,\n",
    "    bcn_img_dir,\n",
    "    batch_size=32,\n",
    "    img_size=256,\n",
    "    train_split=0.7,\n",
    "    val_split=0.15,\n",
    "    test_split=0.15,\n",
    "    top_n_classes=None,\n",
    "    seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Create train, validation, and test data loaders\n",
    "    \n",
    "    Args:\n",
    "        top_n_classes: If specified, only keep the top N most frequent classes\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Loading Datasets\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load both datasets\n",
    "    ham_paths, ham_labels = load_ham10000_metadata(ham_metadata_path, ham_img_part1, ham_img_part2)\n",
    "    bcn_paths, bcn_labels = load_bcn20000_metadata(bcn_metadata_path, bcn_img_dir)\n",
    "    \n",
    "    # Combine\n",
    "    all_paths = ham_paths + bcn_paths\n",
    "    all_labels = ham_labels + bcn_labels\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMBINED: {len(all_paths)} total images\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Show full distribution\n",
    "    print(\"\\nFull class distribution:\")\n",
    "    label_counts = pd.Series(all_labels).value_counts()\n",
    "    for dx, count in label_counts.items():\n",
    "        pct = (count / len(all_labels)) * 100\n",
    "        print(f\"  {dx}: {count} ({pct:.2f}%)\")\n",
    "    \n",
    "    # FILTER TO TOP N CLASSES\n",
    "    if top_n_classes is not None:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"FILTERING TO TOP {top_n_classes} CLASSES\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Get top N classes\n",
    "        top_classes = label_counts.nlargest(top_n_classes).index.tolist()\n",
    "        print(f\"\\nTop {top_n_classes} classes selected:\")\n",
    "        for i, cls in enumerate(top_classes, 1):\n",
    "            print(f\"  {i}. {cls} ({label_counts[cls]} images)\")\n",
    "        \n",
    "        # Filter data\n",
    "        filtered_paths = []\n",
    "        filtered_labels = []\n",
    "        \n",
    "        for path, label in zip(all_paths, all_labels):\n",
    "            if label in top_classes:\n",
    "                filtered_paths.append(path)\n",
    "                filtered_labels.append(label)\n",
    "        \n",
    "        all_paths = filtered_paths\n",
    "        all_labels = filtered_labels\n",
    "        \n",
    "        print(f\"\\nFiltered dataset: {len(all_paths)} images\")\n",
    "        print(\"\\nFiltered distribution:\")\n",
    "        for dx, count in pd.Series(all_labels).value_counts().items():\n",
    "            pct = (count / len(all_labels)) * 100\n",
    "            print(f\"  {dx}: {count} ({pct:.2f}%)\")\n",
    "    \n",
    "    # Create class mapping\n",
    "    unique_diseases = sorted(list(set(all_labels)))\n",
    "    disease_classes = {disease: idx for idx, disease in enumerate(unique_diseases)}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Final: {len(unique_diseases)} classes\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for disease, idx in disease_classes.items():\n",
    "        count = all_labels.count(disease)\n",
    "        pct = (count / len(all_labels)) * 100\n",
    "        print(f\"  [{idx}] {disease}: {count} ({pct:.2f}%)\")\n",
    "    \n",
    "    # Split data\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Creating splits...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
    "        all_paths, all_labels,\n",
    "        test_size=(val_split + test_split),\n",
    "        stratify=all_labels,\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    val_ratio = val_split / (val_split + test_split)\n",
    "    val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
    "        temp_paths, temp_labels,\n",
    "        test_size=(1 - val_ratio),\n",
    "        stratify=temp_labels,\n",
    "        random_state=seed\n",
    "    )\n",
    "    \n",
    "    print(f\"  Train: {len(train_paths)} ({train_split*100:.0f}%)\")\n",
    "    print(f\"  Val:   {len(val_paths)} ({val_split*100:.0f}%)\")\n",
    "    print(f\"  Test:  {len(test_paths)} ({test_split*100:.0f}%)\")\n",
    "    \n",
    "    # Transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = SkinLesionDataset(train_paths, train_labels, disease_classes, train_transform)\n",
    "    val_dataset = SkinLesionDataset(val_paths, val_labels, disease_classes, val_test_transform)\n",
    "    test_dataset = SkinLesionDataset(test_paths, test_labels, disease_classes, val_test_transform)\n",
    "    \n",
    "    # Create loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"✓ Data loaders ready!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, disease_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59cb37ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DATA_DIR: /Users/mrla/Documents/Projects/apan5560-project/\n",
      "\n",
      "============================================================\n",
      "Loading Datasets\n",
      "============================================================\n",
      "HAM metadata columns: ['lesion_id', 'image_id', 'dx', 'dx_type', 'age', 'sex', 'localization', 'dataset']\n",
      "HAM metadata shape: (10015, 8)\n",
      "\n",
      "HAM10000 diagnosis distribution:\n",
      "  Nevus: 6705\n",
      "  Melanoma (HAM): 1113\n",
      "  Benign keratosis (HAM): 1099\n",
      "  Basal cell carcinoma: 514\n",
      "  Actinic keratosis: 327\n",
      "  Vascular lesion: 142\n",
      "  Dermatofibroma: 115\n",
      "\n",
      "BCN20000 metadata shape: (18946, 15)\n",
      "BCN20000 columns: ['isic_id', 'attribution', 'copyright_license', 'age_approx', 'anatom_site_general', 'anatom_site_special', 'concomitant_biopsy', 'diagnosis_1', 'diagnosis_2', 'diagnosis_3', 'diagnosis_confirm_type', 'image_type', 'lesion_id', 'melanocytic', 'sex']\n",
      "\n",
      "BCN20000 loaded: 17639 images\n",
      "  Skipped 1307 images with missing diagnosis (6.9%)\n",
      "BCN20000 diagnosis distribution:\n",
      "  Nevus: 5647\n",
      "  Melanoma (BCN): 4003\n",
      "  Basal cell carcinoma: 3676\n",
      "  Seborrheic keratosis: 1268\n",
      "  Actinic keratosis: 1088\n",
      "  Melanoma metastasis: 633\n",
      "  Squamous cell carcinoma: 559\n",
      "  Scar: 314\n",
      "  Solar lentigo: 283\n",
      "  Dermatofibroma: 168\n",
      "\n",
      "============================================================\n",
      "COMBINED: 27654 total images\n",
      "============================================================\n",
      "\n",
      "Full class distribution:\n",
      "  Nevus: 12352 (44.67%)\n",
      "  Basal cell carcinoma: 4190 (15.15%)\n",
      "  Melanoma (BCN): 4003 (14.48%)\n",
      "  Actinic keratosis: 1415 (5.12%)\n",
      "  Seborrheic keratosis: 1268 (4.59%)\n",
      "  Melanoma (HAM): 1113 (4.02%)\n",
      "  Benign keratosis (HAM): 1099 (3.97%)\n",
      "  Melanoma metastasis: 633 (2.29%)\n",
      "  Squamous cell carcinoma: 559 (2.02%)\n",
      "  Scar: 314 (1.14%)\n",
      "  Dermatofibroma: 283 (1.02%)\n",
      "  Solar lentigo: 283 (1.02%)\n",
      "  Vascular lesion: 142 (0.51%)\n",
      "\n",
      "============================================================\n",
      "FILTERING TO TOP 3 CLASSES\n",
      "============================================================\n",
      "\n",
      "Top 3 classes selected:\n",
      "  1. Nevus (12352 images)\n",
      "  2. Basal cell carcinoma (4190 images)\n",
      "  3. Melanoma (BCN) (4003 images)\n",
      "\n",
      "Filtered dataset: 20545 images\n",
      "\n",
      "Filtered distribution:\n",
      "  Nevus: 12352 (60.12%)\n",
      "  Basal cell carcinoma: 4190 (20.39%)\n",
      "  Melanoma (BCN): 4003 (19.48%)\n",
      "\n",
      "============================================================\n",
      "Final: 3 classes\n",
      "============================================================\n",
      "  [0] Basal cell carcinoma: 4190 (20.39%)\n",
      "  [1] Melanoma (BCN): 4003 (19.48%)\n",
      "  [2] Nevus: 12352 (60.12%)\n",
      "\n",
      "============================================================\n",
      "Creating splits...\n",
      "============================================================\n",
      "  Train: 14381 (70%)\n",
      "  Val:   3082 (15%)\n",
      "  Test:  3082 (15%)\n",
      "\n",
      "============================================================\n",
      "✓ Data loaders ready!\n",
      "============================================================\n",
      "\n",
      "Testing train loader...\n",
      "Batch shape: torch.Size([32, 3, 256, 256])\n",
      "Labels shape: torch.Size([32])\n",
      "Sample labels: ('Basal cell carcinoma', 'Nevus', 'Nevus', 'Melanoma (BCN)', 'Basal cell carcinoma')\n",
      "\n",
      "✓ Everything working correctly!\n",
      "\n",
      "Sample image shape: torch.Size([3, 256, 256])\n",
      "Sample label: 0 (Basal cell carcinoma)\n",
      "\n",
      "First 10 training samples:\n",
      "  [0] Basal cell carcinoma (class 0)\n",
      "  [1] Basal cell carcinoma (class 0)\n",
      "  [2] Melanoma (BCN) (class 1)\n",
      "  [3] Nevus (class 2)\n",
      "  [4] Nevus (class 2)\n",
      "  [5] Nevus (class 2)\n",
      "  [6] Basal cell carcinoma (class 0)\n",
      "  [7] Basal cell carcinoma (class 0)\n",
      "  [8] Melanoma (BCN) (class 1)\n",
      "  [9] Nevus (class 2)\n",
      "\n",
      "Testing train loader with for loop...\n",
      "Batch 0:\n",
      "  Images shape: torch.Size([32, 3, 256, 256])\n",
      "  Labels: tensor([0, 2, 2, 2, 2])\n",
      "  Names: ('Basal cell carcinoma', 'Nevus', 'Nevus', 'Nevus', 'Nevus')\n"
     ]
    }
   ],
   "source": [
    "# Load DATA_DIR from .env file\n",
    "DATA_DIR = os.getenv('DATA_DIR')\n",
    "\n",
    "if DATA_DIR is None:\n",
    "    raise ValueError(\"DATA_DIR not found in .env file. Please create .env with DATA_DIR=<path>\")\n",
    "\n",
    "print(f\"Using DATA_DIR: {DATA_DIR}\\n\")\n",
    "\n",
    "# Define paths\n",
    "ham_metadata = os.path.join(DATA_DIR, \"dataverse_files/HAM10000_metadata\")\n",
    "ham_part1 = os.path.join(DATA_DIR, \"dataverse_files/HAM10000_images_part_1\")\n",
    "ham_part2 = os.path.join(DATA_DIR, \"dataverse_files/HAM10000_images_part_2\")\n",
    "\n",
    "bcn_metadata = os.path.join(DATA_DIR, \"bcn20000_metadata_2025-10-21.csv\")\n",
    "bcn_images = os.path.join(DATA_DIR, \"ISIC-images\")\n",
    "\n",
    "# Create data loaders\n",
    "\n",
    "# Example 1: All classes\n",
    "# train_loader, val_loader, test_loader, disease_classes = create_data_loaders(\n",
    "#     ham_metadata_path=ham_metadata,\n",
    "#     ham_img_part1=ham_part1,\n",
    "#     ham_img_part2=ham_part2,\n",
    "#     bcn_metadata_path=bcn_metadata,\n",
    "#     bcn_img_dir=bcn_images,\n",
    "#     batch_size=32,\n",
    "#     img_size=256,\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "# Example 2: Top 3 classes only (for conditional GAN milestone)\n",
    "train_loader, val_loader, test_loader, disease_classes = create_data_loaders(\n",
    "    ham_metadata_path=ham_metadata,\n",
    "    ham_img_part1=ham_part1,\n",
    "    ham_img_part2=ham_part2,\n",
    "    bcn_metadata_path=bcn_metadata,\n",
    "    bcn_img_dir=bcn_images,\n",
    "    batch_size=32,\n",
    "    img_size=256,\n",
    "    top_n_classes=3,  # Filter to top 3 classes\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Test loading a batch\n",
    "print(\"\\nTesting train loader...\")\n",
    "images, labels, label_names = next(iter(train_loader))\n",
    "print(f\"Batch shape: {images.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Sample labels: {label_names[:5]}\")\n",
    "\n",
    "print(\"\\n✓ Everything working correctly!\")\n",
    "\n",
    "# Get one sample directly from dataset\n",
    "sample_img, sample_label, sample_name = train_loader.dataset[0]\n",
    "print(f\"\\nSample image shape: {sample_img.shape}\")\n",
    "print(f\"Sample label: {sample_label} ({sample_name})\")\n",
    "\n",
    "# Check first 10 samples\n",
    "print(\"\\nFirst 10 training samples:\")\n",
    "for i in range(10):\n",
    "    img, label, name = train_loader.dataset[i]\n",
    "    print(f\"  [{i}] {name} (class {label})\")\n",
    "\n",
    "# Sometimes a for loop works better than next(iter())\n",
    "print(\"\\nTesting train loader with for loop...\")\n",
    "for batch_idx, (images, labels, label_names) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(f\"  Images shape: {images.shape}\")\n",
    "    print(f\"  Labels: {labels[:5]}\")\n",
    "    print(f\"  Names: {label_names[:5]}\")\n",
    "    break  # Only test first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70d56e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f980ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apan5560-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
