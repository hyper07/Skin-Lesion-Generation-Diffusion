# Skin Lesion Generation with GANs and Diffusion Models

Generate synthetic dermoscopic images using 4 different generative models trained on HAM10000 and ISIC datasets.

## Tech Stack

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/release/python-3120/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Diffusers-yellow.svg)](https://huggingface.co/docs/diffusers/index)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.32+-red.svg)](https://streamlit.io/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-009688.svg)](https://fastapi.tiangolo.com/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED.svg)](https://www.docker.com/)
[![Weights & Biases](https://img.shields.io/badge/Weights%20&%20Biases-Experiment%20Tracking-FFBE00.svg)](https://wandb.ai/)
[![Diffusion Models](https://img.shields.io/badge/Diffusion-Models-FF6B6B.svg)](https://en.wikipedia.org/wiki/Diffusion_model)
[![LoRA](https://img.shields.io/badge/LoRA-Fine--Tuning-9C27B0.svg)](https://arxiv.org/abs/2106.09685)

## ğŸ¯ Available Models

| Model | Type | Best For | Training Time |
|-------|------|----------|---------------|
| **CGAN** | Conditional GAN | Fast training, good quality | ~2-3 hours |
| **Conditional Diffusion** | Custom Diffusion | High quality, flexible | ~6-8 hours |
| **Prebuilt GAN** | StyleGAN-based | Fine-tuning, transfer learning | ~4-6 hours |
| **Prebuilt Diffusion** | Stable Diffusion | Best quality, memory efficient | ~8-12 hours |


## UI Screenshots

### Home Page
![UI Home](images/1_main.png)

### Generation Page
![UI Generation](images/2_generation.png)

### Evaluation
![UI Evaluation](images/3_evaluation.png)


## ğŸ“š Full Documentation

**[Complete Project Documentation](https://docs.google.com/document/d/14nC9hbQKUlsImFxmFWOYz6TunfiWvMNqzaboft5dPyE/edit?tab=t.wsanzcdhkh2h)** - Methodology, architecture details, and results


## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Clone repository
git clone https://github.com/mrakelinggar/apan5560-project.git
cd apan5560-project

# Create virtual environment (recommended: uv)
curl -LsSf https://astral.sh/uv/install.sh | sh
uv venv && source .venv/bin/activate
uv sync

# Alternative: using pip
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -e .
```

### 2. Download Dataset

```bash
# Automatic download (recommended)
python download_dataset.py "1KCjAyv1vjB1p92YcMS1rfTcXzFFc4LT0"

# Or manual: https://drive.google.com/file/d/1DmJEVpWF8iLaiF5_ET1G8HRYzhh5tSAD/view
```

### 3. Check Your GPU

```bash
python device_utils.py  # Auto-detects CUDA/MPS/CPU
```

---

## ğŸ“ Training Models

### CGAN (Fastest - Good for Testing)

```bash
python train/train_cgan.py \
    --ham_csv dataset/HAM10000_metadata.csv \
    --ham_img_dir dataset/HAM10000_images \
    --bcn_csv dataset/ISIC_metadata.csv \
    --bcn_img_dir dataset/ISIC_images \
    --batch_size 64 \
    --epochs 50 \
    --img_size 64
```

**Key Parameters:**
- `--img_size`: 32, 64, or 128 (default: 64)
- `--batch_size`: 64 (default)
- `--epochs`: 50+ recommended
- `--amp`: Enable mixed precision (CUDA only)

### Conditional Diffusion (High Quality)

```bash
python train/train_diffusion.py \
    --ham_csv dataset/HAM10000_metadata.csv \
    --ham_img_dir dataset/HAM10000_images \
    --bcn_csv dataset/ISIC_metadata.csv \
    --bcn_img_dir dataset/ISIC_images \
    --batch_size 4 \
    --epochs 100 \
    --img_size 256 \
    --use_lora \
    --amp
```

**Key Parameters:**
- `--img_size`: 128, 256, or 512 (default: 256)
- `--batch_size`: 4-8 recommended (default: 32)
- `--num_timesteps`: 500-1000 (default: 500, higher = better quality)
- `--use_lora`: Enable efficient fine-tuning (recommended)
- `--amp`: Mixed precision for NVIDIA GPUs

### Prebuilt GAN (Transfer Learning)

```bash
python train/train_prebuilt_gan.py \
    --ham_csv dataset/HAM10000_metadata.csv \
    --ham_img_dir dataset/HAM10000_images \
    --bcn_csv dataset/ISIC_metadata.csv \
    --bcn_img_dir dataset/ISIC_images \
    --batch_size 16 \
    --epochs 100 \
    --img_size 128
```

**Key Parameters:**
- `--img_size`: 128 or 256 (default: 128)
- `--batch_size`: 16 recommended (default: 16)
- `--epochs`: 100+ recommended

âš ï¸ **Important:** Use the same `img_size` for training and generation! Provided checkpoint uses 128.

### Prebuilt Diffusion (Best Quality, Stable Diffusion-based)

```bash
python train/train_probability_diffusion.py \
    --ham_csv dataset/HAM10000_metadata.csv \
    --ham_img_dir dataset/HAM10000_images \
    --bcn_csv dataset/ISIC_metadata.csv \
    --bcn_img_dir dataset/ISIC_images \
    --batch_size 8 \
    --epochs 100 \
    --img_size 128 \
    --use_lora \
    --amp
```

**Key Parameters:**
- `--img_size`: Must be multiple of 8 (128, 256, 512)
- `--batch_size`: 4-8 recommended due to memory requirements
- `--use_lora`: Highly recommended for memory efficiency
- Uses pre-trained Stable Diffusion v1.4 (downloads automatically)

**Dataset Configuration:**
- **Unified Data Loader**: All models use the same data loading interface
- **HAM10000 + BCN20000**: Combined datasets with standardized labels
- **Automatic Image Resizing**: All images are automatically resized to square `img_size Ã— img_size` regardless of original resolution
- **No Data Augmentation**: Uses original images without any augmentation for clean training
- **No Class Balancing**: Uses all original images from each class without sampling or capping
- **Stratified Splits**: 70% train, 15% validation, 15% test
- **Class Filtering**: Optional `--top_n_classes` to filter to top N classes (can be added to data loader)

**Training Features:**
- **Automatic Device Detection**: Automatically detects and uses MPS (Mac), CUDA (NVIDIA), or CPU
- **Progress Bars**: Real-time training progress with `tqdm` showing loss metrics
- **Model-Specific Checkpoints**: Each model saves to its own directory
- **Final Model Saving**: Automatically saves final trained models as `<model>_final.pt` for easy loading
- **Training Summaries**: Saves comprehensive training summaries as `training_summary.json` with final losses, epochs, and model configs
- **Output Organization**: Generated samples saved to `output/<model_name>/`
- **LoRA Support**: Efficient fine-tuning for diffusion models (Conditional Diffusion and Prebuilt Diffusion)
- **Adaptive Dataloading**: Auto-tuned workers, pinned memory, and optional AMP for CUDA pipelines
- **Auto Dataset Download**: All training scripts automatically download dataset if missing
- **Error Handling**: Robust image loading with fallback for corrupted images

**Checkpoint & Output Directories:** See "Project Structure" section below for directory organization.

### 7. Generate Images

After training, generate synthetic skin lesion images using the final trained models:

#### Generate with Conditional GAN
```bash
python generate/generate_cgan.py \
    --checkpoint checkpoints/cgan/cgan{img_size}_final.pt \
    --num_samples 8 \
    --include_real \
    --output output/cgan/cgan_generated.png
```

**Generation Parameters:**
- `--checkpoint`: Path to trained model checkpoint
- `--num_samples`: Number of generated samples per class (default: 8)
- `--include_real`: Include one real image per class for comparison
- `--output`: Output path for generated grid
- `--ham_csv`, `--ham_img_dir`, `--bcn_csv`, `--bcn_img_dir`: Dataset paths (for real image loading)

#### Generate with Conditional Diffusion
```bash
python generate/generate_diffusion.py \
    --checkpoint checkpoints/conditional_diffusion/diffusion_final.pt \
    --class_id 0 \
    --num_samples 4 \
    --output output/conditional_diffusion/diffusion_generated.png
```

#### Generate with Prebuilt GAN
```bash
python generate/generate_prebuilt_gan.py \
    --checkpoint checkpoints/prebuilt_gan/G_final.pt \
    --num_classes 3 \
    --img_size 128 \
    --samples_per_class 4 \
    --output output/prebuilt_gan/prebuilt_gan_generated.png
```

**Important:** The `img_size` parameter must match the size used during training. The provided checkpoint (`G_final.pt`) was trained with `img_size=128`. Using a different size will cause a model architecture mismatch error.

#### Generate with Prebuilt Diffusion
```bash
python generate/generate_pb_diffusion.py \
  --checkpoint checkpoints/probability_diffusion/prebuilt_diffusion_epoch_88_final.pt \
  --num_classes 3 \
  --class_id 0 \
  --num_samples 4 \
  --output output/probability_diffusion
```

### 8. Check GPU Acceleration

The project includes an automatic device detection utility that checks for CUDA (NVIDIA), MPS (Apple Silicon), or CPU:

```bash
# Automatic device detection (recommended)
python device_utils.py
```

This will automatically:
- **Detect NVIDIA GPUs** via `nvidia-smi` and PyTorch CUDA
- **Detect Apple Silicon GPUs** (M1/M2/M3) via MPS
- **Fallback to CPU** if no GPU is available
- **Display detailed information** about available devices
- **Test device functionality** with a simple tensor operation

**Output includes:**
- System information (platform, processor, Python, PyTorch versions)
- NVIDIA GPU details (if available via nvidia-smi)
- CUDA availability and GPU information (if available in PyTorch)
- MPS availability (for Apple Silicon)
- CPU information
- Recommended device for training

**Alternative manual checks:**
```bash
# Quick CUDA check
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"

# Quick MPS check (Mac)
python -c "import torch; print(f'MPS available: {torch.backends.mps.is_available() if hasattr(torch.backends, \"mps\") else False}')"

# Check device count
python -c "import torch; print(f'CUDA devices: {torch.cuda.device_count() if torch.cuda.is_available() else 0}')"
```

### 9. Intel PC GPU Setup (Optional)

**If you have an NVIDIA GPU but CUDA isn't working:**

1. **Install NVIDIA Drivers:**
   ```bash
   # Windows: Download from nvidia.com/drivers
   # Linux: Use your package manager
   sudo apt update && sudo apt install nvidia-driver-535  # Ubuntu example
   ```

2. **Install CUDA-enabled PyTorch:**
   ```bash
   # Uninstall CPU-only version first
   pip uninstall torch torchvision torchaudio
   
   # Install CUDA version
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
   ```

3. **Verify Installation:**
   ```bash
   nvidia-smi  # Should show your GPU
   python intel_utils.py  # Should detect CUDA
   ```

## System Requirements

### Minimum Requirements
- **Python**: 3.12 or higher
- **RAM**: 8GB (16GB recommended)
- **Storage**: 10GB free space
- **OS**: macOS 10.15+, Windows 10+, or Linux

### Recommended for Training
- **Mac**: M1/M2/M3 with 16GB+ unified memory
- **Intel PC**: NVIDIA RTX 3070/4060 or better (8GB+ VRAM)
- **AMD PC**: NVIDIA GPU recommended (AMD ROCm support experimental)
- **RAM**: 32GB for large datasets
- **Storage**: SSD with 50GB+ free space

### GPU Requirements by Platform
| Platform | GPU | VRAM | Performance |
|----------|-----|------|-------------|
| Mac M1/M2/M3 | Integrated | 16GB+ | Excellent with MPS |
| Intel + NVIDIA | RTX 3060+ | 8GB+ | Excellent with CUDA |
| Intel + AMD | RX 6600+ | 8GB+ | Limited (CPU fallback) |
| Intel iGPU | Intel Xe | N/A | CPU only |

## Dependencies Overview

Our project includes comprehensive packages for:

### **Core Data Science**
- `numpy`, `pandas`, `matplotlib`, `seaborn`, `scikit-learn`
- `jupyter`, `ipykernel` for interactive development

### **Deep Learning & Computer Vision**
- `torch`, `torchvision`, `torchaudio` - PyTorch ecosystem
- `transformers`, `diffusers` - Hugging Face models
- `timm` - Pre-trained vision models
- `pytorch-lightning` - High-level training framework

### **Image Processing**
- `opencv-python` - Computer vision operations
- `pillow`, `imageio`, `scikit-image` - Image manipulation
- `albumentations` - Advanced data augmentation

### **Performance & GPU Acceleration**
- `accelerate` - Distributed training and mixed precision
- `xformers` - Memory-efficient transformers
- `safetensors` - Safe model serialization

### **Experiment Tracking & Visualization**
- `wandb` - Experiment tracking and collaboration
- `tensorboard` - Training visualization
- `tqdm` - Progress bars

## Project Structure

```
apan5560-project/
â”œâ”€â”€ Root Files
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”œâ”€â”€ uv.lock
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ .env
â”‚   â”œâ”€â”€ .gitignore
â”‚   â”œâ”€â”€ .python-version
â”‚   â”œâ”€â”€ dataset.py                    # Legacy dataset utilities
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ main (archive).py
â”‚   â”œâ”€â”€ download_dataset.py
â”‚   â”œâ”€â”€ download_models.py
â”‚   â”œâ”€â”€ example_download.py
â”‚   â”œâ”€â”€ generate_samples.py
â”‚   â”œâ”€â”€ generate_batch_evaluation.py
â”‚   â”œâ”€â”€ device_utils.py                # CUDA/MPS/CPU detection
â”‚   â”œâ”€â”€ mps_utils.py                   # Apple Silicon utilities
â”‚   â”œâ”€â”€ intel_utils.py                 # Intel PC utilities
â”‚   â”œâ”€â”€ training_config.py
â”‚   â”œâ”€â”€ train_diffusion.py
â”‚   â”œâ”€â”€ diagnose_model.py
â”‚   â”œâ”€â”€ debug_diffusers.py
â”‚   â”œâ”€â”€ fix_macos_mutex.sh
â”‚   â””â”€â”€ GENERATED_IMAGES_ORGANIZATION.md
â”‚
â”œâ”€â”€ Data & Notebooks
â”‚   â”œâ”€â”€ dataset/                       # HAM10000 + ISIC datasets
â”‚   â”‚   â”œâ”€â”€ HAM10000_metadata.csv
â”‚   â”‚   â”œâ”€â”€ HAM10000_images/
â”‚   â”‚   â”œâ”€â”€ HAM10000_segmentations_lesion_tschandl/
â”‚   â”‚   â”œâ”€â”€ ISIC_metadata.csv
â”‚   â”‚   â”œâ”€â”€ ISIC_images/
â”‚   â”‚   â”œâ”€â”€ ISIC2018_Task3_Test_Images/
â”‚   â”‚   â”œâ”€â”€ Test/
â”‚   â”‚   â””â”€â”€ readme.txt
â”‚   â”œâ”€â”€ Train Test Val.ipynb
â”‚   â”œâ”€â”€ Train Test Val (Top 3).ipynb
â”‚   â”œâ”€â”€ Exploration/
â”‚   â”‚   â””â”€â”€ Metadata Explorer.ipynb
â”‚   â””â”€â”€ mockups/
â”‚
â”œâ”€â”€ Data Loading
â”‚   â””â”€â”€ data/                          # Unified data loader
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ data_loader.py             # HAM10000 + BCN20000
â”‚       â”œâ”€â”€ dataset.py
â”‚       â””â”€â”€ dataset_utils.py
â”‚
â”œâ”€â”€ Models
â”‚   â””â”€â”€ models/                        # Model architectures
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ cgan.py
â”‚       â”œâ”€â”€ conditional_diffusion.py   # With LoRA support
â”‚       â”œâ”€â”€ ConditionalDiffusion.md
â”‚       â”œâ”€â”€ prebuilt_gan.py
â”‚       â”œâ”€â”€ prebuilt_diffusion.py      # Stable Diffusion
â”‚       â”œâ”€â”€ data_loader.py
â”‚       â”œâ”€â”€ dataset.py
â”‚       â”œâ”€â”€ generate.py
â”‚       â”œâ”€â”€ model.py
â”‚       â””â”€â”€ train_diffusion.py
â”‚
â”œâ”€â”€ Training
â”‚   â””â”€â”€ train/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ train_cgan.py
â”‚       â”œâ”€â”€ train_diffusion.py
â”‚       â”œâ”€â”€ train_prebuilt_gan.py
â”‚       â”œâ”€â”€ train_prebuilt_diffusion.py
â”‚       â”œâ”€â”€ train_probability_diffusion.py
â”‚       â””â”€â”€ finetune_diffusion.py
â”‚
â”œâ”€â”€ Generation
â”‚   â””â”€â”€ generate/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ generate_all.py
â”‚       â”œâ”€â”€ generate_cgan.py
â”‚       â”œâ”€â”€ generate_cgan_prod.py
â”‚       â”œâ”€â”€ generate_diffusion.py
â”‚       â”œâ”€â”€ generate_diffusion_prod.py
â”‚       â”œâ”€â”€ generate_prebuilt_gan.py
â”‚       â”œâ”€â”€ generate_prebuilt_gan_prod.py
â”‚       â”œâ”€â”€ generate_prebuilt_diffusion.py
â”‚       â”œâ”€â”€ generate_prebuilt_diffusion_prod.py
â”‚       â”œâ”€â”€ generate_pb_diffusion.py
â”‚       â””â”€â”€ generate_probability_diffusion.py
â”‚
â”œâ”€â”€ Evaluation
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ cnn_classifier.py
â”‚       â””â”€â”€ evaluate_generated.py
â”‚
â”œâ”€â”€ Checkpoints & Output
â”‚   â”œâ”€â”€ checkpoints/                   # Model checkpoints
â”‚   â”‚   â”œâ”€â”€ prebuilt_diffusion_epoch_88_final.pt
â”‚   â”‚   â”œâ”€â”€ cgan/
â”‚   â”‚   â”œâ”€â”€ conditional_diffusion/
â”‚   â”‚   â”œâ”€â”€ prebuilt_diffusion/
â”‚   â”‚   â”œâ”€â”€ prebuilt_gan/
â”‚   â”‚   â””â”€â”€ Test/
â”‚   â”œâ”€â”€ output/                        # Generated samples
â”‚   â”‚   â”œâ”€â”€ cgan/
â”‚   â”‚   â”œâ”€â”€ conditional_diffusion/
â”‚   â”‚   â”œâ”€â”€ prebuilt_gan/
â”‚   â”‚   â”œâ”€â”€ probability_diffusion/
â”‚   â”‚   â””â”€â”€ evaluation_images/
â”‚   â”œâ”€â”€ lora_weights/                  # LoRA adapters
â”‚   â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ samples/
â”‚   â”œâ”€â”€ results/
â”‚   â””â”€â”€ temp/
â”‚
â”œâ”€â”€ Logs & Monitoring
â”‚   â”œâ”€â”€ logs/                          # Training logs
â”‚   â”‚   â”œâ”€â”€ cnn_evaluator_*/
â”‚   â”‚   â””â”€â”€ eval/
â”‚   â””â”€â”€ log_eval/                      # Evaluation logs
â”‚       â”œâ”€â”€ cgan/
â”‚       â”œâ”€â”€ conditional_diffusion/
â”‚       â”œâ”€â”€ prebuilt_diffusion/
â”‚       â””â”€â”€ prebuilt_gan/
â”‚
â”œâ”€â”€ API Service
â”‚   â””â”€â”€ api/                           # FastAPI service
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â”œâ”€â”€ start.sh
â”‚       â”œâ”€â”€ pyproject.toml
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ download_dataset.py
â”‚       â”œâ”€â”€ download_models.py
â”‚       â”œâ”€â”€ routers/                   # API endpoints
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ embedding.py
â”‚       â”‚   â”œâ”€â”€ neural_networks.py
â”‚       â”‚   â””â”€â”€ probability.py
â”‚       â”œâ”€â”€ commands/                  # CLI commands
â”‚       â”‚   â”œâ”€â”€ train_cnn.py
â”‚       â”‚   â”œâ”€â”€ train_diffusion.py
â”‚       â”‚   â”œâ”€â”€ train_energy.py
â”‚       â”‚   â””â”€â”€ train_gan.py
â”‚       â”œâ”€â”€ help_lib/                  # Helper utilities
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ checkpoints.py
â”‚       â”‚   â”œâ”€â”€ data_loader.py
â”‚       â”‚   â”œâ”€â”€ embeddings.py
â”‚       â”‚   â”œâ”€â”€ evaluator.py
â”‚       â”‚   â”œâ”€â”€ generator.py
â”‚       â”‚   â”œâ”€â”€ model.py
â”‚       â”‚   â”œâ”€â”€ neural_networks.py
â”‚       â”‚   â”œâ”€â”€ probability.py
â”‚       â”‚   â”œâ”€â”€ text_processing.py
â”‚       â”‚   â”œâ”€â”€ trainer.py
â”‚       â”‚   â””â”€â”€ utils.py
â”‚       â”œâ”€â”€ models/                    # API models
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ bigram_model.py
â”‚       â”‚   â”œâ”€â”€ cnn_models.py
â”‚       â”‚   â”œâ”€â”€ energy_diffusion_models.py
â”‚       â”‚   â”œâ”€â”€ gan_models.py
â”‚       â”‚   â”œâ”€â”€ requests.py
â”‚       â”‚   â””â”€â”€ responses.py
â”‚       â”œâ”€â”€ checkpoints/               # API checkpoints
â”‚       â”‚   â”œâ”€â”€ cnn/
â”‚       â”‚   â”œâ”€â”€ cnn_cifar/
â”‚       â”‚   â”œâ”€â”€ cnn_cifar10/
â”‚       â”‚   â”œâ”€â”€ diffusion_cifar/
â”‚       â”‚   â”œâ”€â”€ energy_cifar/
â”‚       â”‚   â”œâ”€â”€ gan/
â”‚       â”‚   â””â”€â”€ gan_mnist/
â”‚       â”œâ”€â”€ dataset/
â”‚       â”œâ”€â”€ data/                     # CIFAR-10, MNIST
â”‚       â”œâ”€â”€ generate/
â”‚       â”œâ”€â”€ lora_weights/
â”‚       â”œâ”€â”€ outputs/
â”‚       â””â”€â”€ results/
â”‚
â”œâ”€â”€ Streamlit App
â”‚   â””â”€â”€ app-streamlit/                 # Web interface
â”‚       â”œâ”€â”€ Home.py
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â”œâ”€â”€ requirements-dev.txt
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ README_GENERATION.md
â”‚       â”œâ”€â”€ localazy.example.json
â”‚       â”œâ”€â”€ utils.py
â”‚       â”œâ”€â”€ pages/
â”‚       â”‚   â”œâ”€â”€ 0_Dataset_Analysis.py
â”‚       â”‚   â”œâ”€â”€ 1_Training.py
â”‚       â”‚   â”œâ”€â”€ 2_Image_Generation.py
â”‚       â”‚   â””â”€â”€ 4_Evaluation.py
â”‚       â”œâ”€â”€ components/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ card.py
â”‚       â”‚   â”œâ”€â”€ footer.py
â”‚       â”‚   â””â”€â”€ header.py
â”‚       â”œâ”€â”€ functions/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ components.py
â”‚       â”‚   â”œâ”€â”€ database.py
â”‚       â”‚   â”œâ”€â”€ eda_components.py
â”‚       â”‚   â”œâ”€â”€ menu.py
â”‚       â”‚   â”œâ”€â”€ model_utils.py
â”‚       â”‚   â”œâ”€â”€ readme.md
â”‚       â”‚   â””â”€â”€ visualization.py
â”‚       â”œâ”€â”€ styles/
â”‚       â”‚   â””â”€â”€ app.css
â”‚       â”œâ”€â”€ locales/                  # Translations (de, pl)
â”‚       â”‚   â”œâ”€â”€ base.pot
â”‚       â”‚   â”œâ”€â”€ de/LC_MESSAGES/base.po
â”‚       â”‚   â””â”€â”€ pl/LC_MESSAGES/base.po
â”‚       â”œâ”€â”€ images/
â”‚       â”œâ”€â”€ data/
â”‚       â”‚   â”œâ”€â”€ summary.csv
â”‚       â”‚   â””â”€â”€ summary.json
â”‚       â”œâ”€â”€ temp/
â”‚       â””â”€â”€ backup/
â”‚
â””â”€â”€ Cache & Python
    â”œâ”€â”€ __pycache__/
    â”œâ”€â”€ .venv/                        # Virtual environment
    â””â”€â”€ .uv/                          # UV cache
```

## Usage Examples

### Using the Unified Data Loader
```python
from data.data_loader import create_data_loaders

# Load data for any model
train_loader, val_loader, test_loader, disease_classes = create_data_loaders(
    ham_metadata_path="dataset/HAM10000_metadata.csv",
    ham_img_dir="dataset/HAM10000_images",
    bcn_metadata_path="dataset/ISIC_metadata.csv",
    bcn_img_dir="dataset/ISIC_images",
    batch_size=32,
    img_size=256,  # Images will be resized to 256Ã—256 square
    top_n_classes=3,  # Optional: filter to top N classes
    seed=42
)
```

**Data Loader Features:** See "Dataset Configuration" section above for details.

### Using Models
```python
from models import (
    Generator, Discriminator,  # Conditional GAN
    ConditionalDiffusionModel,  # Conditional Diffusion
    ConditionalGenerator, ConditionalDiscriminator,  # Prebuilt GAN
    DiffusionModel  # Prebuilt Diffusion
)

# Example 1: Create Conditional Diffusion Model with LoRA
model = ConditionalDiffusionModel(
    image_size=256,
    num_classes=3,
    use_lora=True,  # Enable LoRA
    lora_r=4,
    lora_alpha=16,
    lora_dropout=0.1
)

# Example 2: Create Prebuilt Diffusion Model with LoRA
model = DiffusionModel(
    num_classes=3,
    use_lora=True  # Enable LoRA (config is hardcoded in model)
)
```

### Download Dataset Programmatically
```python
from example_download import download_dataset

# Download with Google Drive URL
success = download_dataset(
    "https://drive.google.com/file/d/YOUR_FILE_ID/view",
    "skin_lesion_data.zip"
)
```

### GPU-Accelerated Training
```python
from training_config import TrainingConfig

# Automatically detects and configures MPS/CUDA
config = TrainingConfig(
    batch_size=64,
    learning_rate=1e-3,
    epochs=10
)

# Your model automatically uses optimal device
model = MyModel()
model = config.move_to_device(model)

# Optimized data loading
train_loader = config.create_dataloader(train_dataset)
```

### Check System Capabilities

**Unified Device Detection (Recommended):**
```bash
python device_utils.py  # Auto-detects CUDA/MPS/CPU (see section above)
```

**Platform-Specific Utilities:**
- **Mac users**: `python mps_utils.py` for detailed MPS information
- **Intel PC users**: `python intel_utils.py` for CUDA/CPU system info

## Dataset Information

This project analyzes dermoscopic images for skin lesion classification using:

- **HAM10000**: 10,000+ dermoscopic images with ground truth labels
- **ISIC2018/BCN20000**: Additional test sets and metadata
- **Image Types**: `.jpg` dermoscopic images (original resolutions vary: 600x450 to 1024x768)
- **Classes**: Multiple skin lesion types (melanoma, nevus, etc.)

**Note:** See "Dataset Configuration" section above for data loading details (resizing, splits, etc.).

### Expected Directory Structure (After Download)
```
dataset/
â”œâ”€â”€ HAM10000_metadata.csv
â”œâ”€â”€ HAM10000_images/          # All HAM10000 images in one folder
â”‚   â”œâ”€â”€ ISIC_0024808.jpg
â”‚   â”œâ”€â”€ ISIC_0024820.jpg
â”‚   â””â”€â”€ ... [thousands more images]
â”œâ”€â”€ ISIC_metadata.csv
â”œâ”€â”€ ISIC_images/              # BCN20000 images
â”‚   â”œâ”€â”€ ISIC_0058528.jpg
â”‚   â”œâ”€â”€ ISIC_0056505.jpg
â”‚   â””â”€â”€ ... [thousands more images]
â””â”€â”€ [other dataset files]
```

## Performance Optimization

### Mac M1/M2/M3 Users
- **Automatic MPS Detection**: GPU acceleration without configuration
- **Optimized Settings**: Batch sizes and memory management tuned for Apple Silicon
- **Mixed Precision**: Automatic FP16 training for faster performance
- **Memory Efficiency**: Unified memory architecture optimization

### Intel PC + NVIDIA GPU Users
- **CUDA Support**: Automatic detection and configuration
- **Mixed Precision**: AMP (Automatic Mixed Precision) with GradScaler
- **Multi-GPU**: Ready for distributed training (DataParallel/DistributedDataParallel)
- **Memory Management**: Optimal GPU memory allocation and cleanup
- **Batch Size Scaling**: Larger batch sizes supported with dedicated VRAM

### Intel PC CPU-Only
- **Optimized Threading**: Multi-core utilization (Intel MKL integration)
- **SIMD Instructions**: AVX2/AVX-512 acceleration when available
- **Memory Management**: Efficient data loading and processing
- **Model Quantization**: INT8 inference for faster CPU performance

### Performance Benchmarks (Typical)
| Hardware | Training Speed | Batch Size | Memory Usage |
|----------|---------------|------------|--------------|
| Mac M1 Pro (16GB) | ~45 img/sec | 32-64 | 12GB |
| RTX 4060 (8GB) | ~85 img/sec | 64-128 | 7GB |
| RTX 3080 (10GB) | ~120 img/sec | 128-256 | 9GB |
| Intel i7 CPU | ~8 img/sec | 16-32 | 8GB |

## Development Workflow

1. **Explore Data**: Start with `Exploration/Metadata Explorer.ipynb`
2. **Check Device**: Run `python device_utils.py` to automatically detect CUDA/MPS/CPU
3. **Train Models**: Use training scripts in `train/` directory
   - `train/train_cgan.py` - Conditional GAN
   - `train/train_diffusion.py` - Conditional Diffusion (with LoRA support)
   - `train/train_prebuilt_gan.py` - Prebuilt GAN
   - `train/train_probability_diffusion.py` - Prebuilt Diffusion (Stable Diffusion based, with LoRA support)
4. **Generate Samples**: Use generation scripts in `generate/` directory
5. **Scale Up**: Use `training_config.py` for production training
6. **Track Experiments**: Connect to Weights & Biases for monitoring

---

## ğŸŒ FastAPI Service

Start the API server for programmatic image generation:

```bash
python main.py  # Server at http://localhost:8000
```

**Check Available Models:**
```bash
curl http://localhost:8000/model_status
```

**Generate Images:**
```bash
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "cgan",
    "class_ids": [0, 4, 7],
    "num_samples": 50
  }'
```

**Available Models:** `cgan`, `conditional_diffusion`, `prebuilt_gan`, `prebuilt_diffusion`

Generated images saved to: `output/{model}/{timestamp}/{class_name}/img_001.png`

---

## ğŸ³ Docker Deployment

```bash
# Start services (Streamlit + FastAPI)
docker-compose up --build

# Access applications
# Streamlit: http://localhost:58502
# FastAPI: http://localhost:18888
```

---

## ğŸ”§ Troubleshooting

**Out of Memory:**
- Reduce `--batch_size`
- Enable `--use_lora` for diffusion models
- Use `--amp` flag for NVIDIA GPUs

**Slow Training:**
- Enable `--amp` for NVIDIA GPUs
- Reduce `--num_timesteps` for diffusion (500 vs 1000)
- Use smaller `--img_size`

**macOS Mutex Error (Prebuilt Diffusion only):**
- Use Linux/Windows or cloud GPU (Google Colab, AWS)
- Or use other models (CGAN, Conditional Diffusion, Prebuilt GAN)

**CUDA Not Working:**
```bash
nvidia-smi  # Check GPU
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

---

## License

APAN5560 coursework. Dataset usage follows ISIC and HAM10000 licensing terms.

---

